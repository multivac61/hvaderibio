on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *" # Run every hour
name: Deploy
# TODO: Figure out a way not to have to write the scraped data to the repo...
permissions:
  contents: write
jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: DeterminateSystems/nix-installer-action@main
      - uses: DeterminateSystems/magic-nix-cache-action@main
      - uses: actions/checkout@main
        with:
          persist-credentials: false

      - name: ğŸ“š Install node dependencies
        run: nix develop -c pnpm install

      - name: ğŸ•·ï¸ Scrape
        run: nix develop -c pnpm scrape

      - name: ğŸ”¨ Build
        run: nix develop -c pnpm build

      - name: Deploy with gh-pages
        run: |
          git remote set-url origin https://git:${GITHUB_TOKEN}@github.com/${GITHUB_REPOSITORY}.git
          nix develop -c pnpx gh-pages -d build -u "github-actions-bot <support+actions@github.com>"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
